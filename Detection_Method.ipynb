{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c564b674",
   "metadata": {},
   "source": [
    "# Attack Detection\n",
    "\n",
    "## 1. AutoEncoder Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7947aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import App.dataset as dataset\n",
    "from App.config import BATCH_SIZE\n",
    "\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "import psutil\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af360bf3",
   "metadata": {},
   "source": [
    "### Desactivation de la GPU pour l'inférence sur la CPU uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c941531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU désactivée.\n"
     ]
    }
   ],
   "source": [
    "# Desactivation de la GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Désactivez toutes les GPU\n",
    "        tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "        print(\"GPU désactivée.\")\n",
    "    except RuntimeError as e:\n",
    "        # La visibilité des GPU ne peut être modifiée qu'avant l'initialisation de TensorFlow\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Aucune GPU disponible, utilisation du CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01fb106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load le modèle\n",
    "model = tf.keras.models.load_model('bev_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74109c1",
   "metadata": {},
   "source": [
    "### Charge les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323fe457",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.create_bev_dataset_from_folder(\n",
    "    folder_path=\"Data/dataset/sequences/00/velodyne_with_obstacles\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1613685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui detecte si oui ou non il y a un obstacle dans l'image en utilisant le modèle AutoEncoder\n",
    "\n",
    "def detect_obstacle(model, image, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Détecte la présence d'un obstacle dans une image en comparant l'image originale avec l'image reconstruite.\n",
    "    \n",
    "    :param model: Modèle AutoEncoder chargé.\n",
    "    :param image: Image à analyser (hauteur, largeur, canaux).\n",
    "    :param threshold: Seuil pour la détection d'obstacle.\n",
    "    :return: Booléen indiquant la présence d'un obstacle.\n",
    "    \"\"\"\n",
    "    # Redimensionner l'image pour correspondre à l'entrée du modèle\n",
    "    image = np.expand_dims(image, axis=0)  # Ajouter une dimension batch\n",
    "    start_time = time.time()\n",
    "    reconstructed_image = model.predict(image)\n",
    "    end_time = time.time()\n",
    "    print (f\"Temps d'inférence : {end_time - start_time:.4f} secondes\")\n",
    "\n",
    "    # Calculer la différence entre l'image originale et l'image reconstruite\n",
    "    difference = np.abs(image - reconstructed_image)\n",
    "    print (f\"Différence moyenne : {np.mean(difference)}\")\n",
    "\n",
    "    # Vérifier si la différence dépasse le seuil\n",
    "    if np.mean(difference) > threshold:\n",
    "        return True  # Obstacle détecté\n",
    "    else:\n",
    "        return False  # Pas d'obstacle détecté\n",
    "\n",
    "# Fonction qui calcule le temps d'inférence d'un batch d'image\n",
    "def infer_obstacle_detection(model, dataset):\n",
    "    \"\"\"\n",
    "    Effectue une inférence de détection d'obstacle sur un dataset.\n",
    "    \n",
    "    :param model: Modèle AutoEncoder chargé.\n",
    "    :param dataset: Dataset TensorFlow pour l'inférence.\n",
    "    :return: Liste des résultats de détection d'obstacle.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    cpus = []\n",
    "\n",
    "    for images, _ in dataset.take(1):\n",
    "        for image in images:\n",
    "            cpu_before = psutil.cpu_percent(interval=None)\n",
    "            obstacle_detected = detect_obstacle(model, image.numpy())\n",
    "            cpu_after = psutil.cpu_percent(interval=None)\n",
    "            results.append(obstacle_detected)\n",
    "            cpus.append(cpu_after - cpu_before)\n",
    "    print (\"CPU USAGE Mean : \", np.mean(cpus))\n",
    "\n",
    "    return results  \n",
    "\n",
    "# Exemple d'utilisation de la fonction de détection d'obstacle\n",
    "\n",
    "def example_obstacle_detection(model, dataset, num_images=5):\n",
    "    \"\"\"\n",
    "    Exemple d'utilisation de la fonction de détection d'obstacle sur un dataset.\n",
    "    \n",
    "    :param model: Modèle AutoEncoder chargé.\n",
    "    :param dataset: Dataset TensorFlow pour l'inférence.\n",
    "    :param num_images: Nombre d'images à analyser.\n",
    "    \"\"\"\n",
    "    for images, _ in dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            image = images[i].numpy()\n",
    "            obstacle_detected = detect_obstacle(model, image)\n",
    "            print(f\"Image {i + 1}: Obstacle détecté ? {'Oui' if obstacle_detected else 'Non'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726474f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_obstacle_detection(\n",
    "    model=model,\n",
    "    dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962be75",
   "metadata": {},
   "source": [
    "## 2. Ruled Based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour injecter des obstacles dans un nuage de points LiDAR\n",
    "def inject_obstacles(lidar_points, num_obstacles=1, obstacle_size=1.0, density=1000, bounds=None):\n",
    "    \"\"\"\n",
    "    Injecte un nombre donné d'obstacles dans un nuage de points LiDAR.\n",
    "    \n",
    "    :param lidar_points: Nuage de points original (numpy array de forme (N, 4) [x, y, z, intensité]).\n",
    "    :param num_obstacles: Nombre d'obstacles à injecter.\n",
    "    :param obstacle_size: Taille de chaque obstacle (rayon en mètres).\n",
    "    :param density: Nombre de points par obstacle.\n",
    "    :param bounds: Limites pour l'injection des obstacles (x_min, x_max, y_min, y_max, z_min, z_max).\n",
    "                   Si None, les limites sont calculées à partir des points existants.\n",
    "    :return: Nuage de points modifié avec les obstacles injectés.\n",
    "    \"\"\"\n",
    "    # Calculer les limites si elles ne sont pas fournies\n",
    "    if bounds is None:\n",
    "        x_min, x_max = np.min(lidar_points[:, 0]), np.max(lidar_points[:, 0])\n",
    "        y_min, y_max = np.min(lidar_points[:, 1]), np.max(lidar_points[:, 1])\n",
    "        z_min, z_max = np.min(lidar_points[:, 2]), np.max(lidar_points[:, 2])\n",
    "        \n",
    "    else:\n",
    "        x_min, x_max, y_min, y_max, z_min, z_max = bounds\n",
    "\n",
    "    # Liste pour stocker les nouveaux points\n",
    "    new_points = []\n",
    "\n",
    "    for _ in range(num_obstacles):\n",
    "        # Générer un centre aléatoire pour l'obstacle\n",
    "        center_x = np.random.uniform(x_min, x_max)\n",
    "        center_y = np.random.uniform(y_min, y_max)\n",
    "        center_z = np.random.uniform(z_min, z_max)\n",
    "\n",
    "        # Générer des points autour du centre\n",
    "        obstacle_points = np.random.uniform(\n",
    "            low=[center_x - obstacle_size, center_y - obstacle_size, center_z - obstacle_size, 0.1],\n",
    "            high=[center_x + obstacle_size, center_y + obstacle_size, center_z + obstacle_size, 1.0],\n",
    "            size=(density, 4)\n",
    "        )\n",
    "        new_points.append(obstacle_points)\n",
    "\n",
    "    # Ajouter les nouveaux points au nuage de points existant\n",
    "    new_points = np.vstack(new_points)\n",
    "    lidar_points_with_obstacles = np.vstack((lidar_points, new_points))\n",
    "\n",
    "    return lidar_points_with_obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70853f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that read all the bin file, add obstacles and save them back into bin\n",
    "def process_lidar_files_with_obstacles(source_dir, target_dir, num_obstacles=1, obstacle_size=1.0, density=50):\n",
    "    \"\"\"\n",
    "    Traite tous les fichiers LiDAR dans un dossier, injecte des obstacles et enregistre les résultats.\n",
    "    \n",
    "    :param source_dir: Dossier source contenant les fichiers .bin LiDAR.\n",
    "    :param target_dir: Dossier cible pour enregistrer les fichiers modifiés.\n",
    "    :param num_obstacles: Nombre d'obstacles à injecter dans chaque nuage de points.\n",
    "    :param obstacle_size: Taille de chaque obstacle (rayon en mètres).\n",
    "    :param density: Nombre de points par obstacle.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.bin'):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            lidar_points = dataset.read_kitti_lidar(file_path)\n",
    "            lidar_points_with_obstacles = inject_obstacles(\n",
    "                lidar_points,\n",
    "                num_obstacles=num_obstacles,\n",
    "                obstacle_size=obstacle_size,\n",
    "                density=density\n",
    "            )\n",
    "            target_file_path = os.path.join(target_dir, filename)\n",
    "            lidar_points_with_obstacles.astype(np.float32).tofile(target_file_path)\n",
    "\n",
    "# Exemple d'utilisation de la fonction pour traiter les fichiers LiDAR\n",
    "source_dir = \"Data/dataset/sequences/02/velodyne/\"\n",
    "target_dir = \"Data/dataset/sequences/02/velodyne_with_obstacles/\"\n",
    "process_lidar_files_with_obstacles(\n",
    "    source_dir=source_dir,\n",
    "    target_dir=target_dir,\n",
    "    num_obstacles=10,\n",
    "    obstacle_size=2.0,\n",
    "    density=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ed699bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test dataset with obstacles\n",
    "test_dataset_with_obstacles = dataset.create_bev_dataset_from_folder(\n",
    "    folder_path=\"Data/dataset/sequences/02/velodyne_with_obstacles\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326277ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "import psutil\n",
    "\n",
    "\n",
    "# Fonction qui utilise des règles pour la detection d'anomalies dans un lot d'images BEV\n",
    "def rule_based_anomaly_detection(bev_batch, intensity_threshold=0.1, density_threshold=0.5, min_cluster_size=10000):\n",
    "    \"\"\"\n",
    "    Détecte les anomalies dans un lot d'images BEV en utilisant des règles basées sur l'intensité et la densité.\n",
    "    \n",
    "    :param bev_batch: Lot d'images BEV (numpy array de forme [batch_size, channels, height, width]).\n",
    "    :param intensity_threshold: Seuil minimal pour considérer un point comme valide (canal intensité).\n",
    "    :param density_threshold: Seuil minimal pour considérer un point comme valide (canal densité).\n",
    "    :param min_cluster_size: Taille minimale d'un cluster pour qu'il soit considéré comme valide.\n",
    "    :return: Liste des indices des images contenant des anomalies.\n",
    "    \"\"\"\n",
    "    anomaly_indices = []\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, bev in enumerate(bev_batch):\n",
    "        valid_points = (bev[0] > intensity_threshold) & (bev[2] > density_threshold)\n",
    "        labeled_clusters, num_clusters = label(valid_points)\n",
    "        # Vérifier la taille des clusters\n",
    "        for cluster_id in range(1, num_clusters + 1):\n",
    "            cluster_size = np.sum(labeled_clusters == cluster_id)\n",
    "            if cluster_size < min_cluster_size:\n",
    "                anomaly_indices.append(i)\n",
    "                break  # Anomalie détectée dans cette image\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return anomaly_indices, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2127633",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "times = []\n",
    "cpu_usasges = []\n",
    "\n",
    "for images, _ in test_dataset_with_obstacles.take(1):\n",
    "    # Convertir les images en numpy array\n",
    "    bev_batch = images.numpy()\n",
    "    # Détecter les anomalies dans le lot d'images en utilisant des règles\n",
    "\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    anomalies_detected = rule_based_anomaly_detection(\n",
    "        bev_batch=bev_batch,\n",
    "        intensity_threshold=0.5,\n",
    "        density_threshold=0.5,\n",
    "        min_cluster_size=10000\n",
    "    )\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    memory_usage_in_mb = memory_usage((rule_based_anomaly_detection, (bev_batch, 0.5, 0.5, 10000)), max_usage=True)\n",
    "    print (f\"Utilisation maximale de la mémoire : {memory_usage_in_mb} MB\")\n",
    "\n",
    "    times.append(anomalies_detected[1])\n",
    "    cpu_usasges.append(cpu_after - cpu_before)\n",
    "\n",
    "    for i in anomalies_detected[0]:\n",
    "        counter += 1\n",
    "        print(f\"Anomalie détectée dans l'image {i + 1} du lot.\")\n",
    "\n",
    "print(f\"Nombre total d'anomalies détectées dans le lot : {counter}/{len(bev_batch)}\")\n",
    "print(f\"Temps moyen de détection d'anomalies par image (CPU): {np.mean(cpu_usasges):.4f} %\")\n",
    "print (f\"Temps moyen de détection d'anomalies par image : {np.mean(times):.4f} secondes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
